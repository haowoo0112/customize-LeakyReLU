# customize-LeakyReLU  

## What is LeakyReLU  

Leaky ReLU function is an improved version of the ReLU activation function. As for the ReLU activation function, the gradient is 0 for all the values of inputs that are less than zero, which would deactivate the neurons in that region and may cause dying ReLU problem.  

![](https://i.imgur.com/MSF6x7t.png)  